{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Aufgabe: Crawler\n",
    "\n",
    "**Aufgabe:**\n",
    "\n",
    "- Passe den ArticleFetcher so an, dass er die Informationen aus allen Seiten extrahiert\n",
    "\n",
    "Hier nochmal die URL: http://python.beispiel.programmierenlernen.io/index.php\n",
    "\n",
    "**Tipps:**\n",
    "\n",
    "- Schau dir zuerst an, wie du den Button \"Zur nächsten Seite\" ansteuern kannst.\n",
    "- Wie greifst du von Python aus auf das \"href\" - Attribut dieses Buttons zu?\n",
    "- (Optional): Probier ggf. zuerst, nur die Infos der ersten 2 Seiten zu holen. Kannst du darauf aufbauend das Programm verallgemeinern, so dass es alle Seiten einliest?\n",
    "- Du kannst dich daran orientieren, ob es einen \"Zur nächsten Seite\"-Button gibt, oder nicht. Wenn es diesen Button nicht mehr gibt, bist du auf der letzten Seite angelangt. Welcher Schleifentyp bietet sich hier an, wenn du die Schleife erst dann stoppen willst, wenn es den Button nicht mehr gibt?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from urllib.parse import urljoin\r\n",
    "\r\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class CrawledArticle():\r\n",
    "\r\n",
    "    def __init__(self, title, emoji, content, image):\r\n",
    "        self.title = title\r\n",
    "        self.emoji = emoji\r\n",
    "        self.content = content\r\n",
    "        self.image = image\r\n",
    "\r\n",
    "    def __str__(self):\r\n",
    "        return self.emoji + \": \" + self.title\r\n",
    "\r\n",
    "    def to_csv(self):\r\n",
    "        return [self.emoji, self.title, self.content, self.image]\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def csv_header():\r\n",
    "        return [\"Emoji\", \"Title\", \"Content\", \"ImageUrl\"]\r\n",
    "\r\n",
    "class NextPage():\r\n",
    "\r\n",
    "    def __init__(self, url, links):\r\n",
    "        self.has_next_page = False\r\n",
    "        self.links = links\r\n",
    "        for link in links:\r\n",
    "             if link.text == \"Zur nächsten Seite!\":\r\n",
    "                self.url_args  = {}\r\n",
    "                for arg in link.attrs[\"href\"].split('?'):\r\n",
    "                    if '=' in arg:\r\n",
    "                        (k, v) = arg.split(\"=\")\r\n",
    "                        self.url_args[k] = v\r\n",
    "                self.has_next_page = \"page\" in self.url_args\r\n",
    "        self.url = urljoin(url, f\"?page={self.url_args['page']}\") if self.has_next_page else None\r\n",
    "\r\n",
    "    def __str__(self):\r\n",
    "        return self.url\r\n",
    "\r\n",
    "class ArticleFetcher():\r\n",
    "    def __init__(self, base_url):\r\n",
    "        self.url = base_url\r\n",
    "    \r\n",
    "    def fetch(self, limit_pages = 0, limit_entries = 0):\r\n",
    "        url = self.url\r\n",
    "        page_count = 0\r\n",
    "        entry_count = 0\r\n",
    "\r\n",
    "        while(url != None):\r\n",
    "            if limit_pages > 0 and page_count >= limit_pages:\r\n",
    "                break\r\n",
    "            time.sleep(1)\r\n",
    "            print(\"read data from \" + url)\r\n",
    "            r = requests.get(url)\r\n",
    "            doc = BeautifulSoup(r.text, \"html.parser\")\r\n",
    "            next_page = NextPage(url, doc.select(\"div.navigation a\"))\r\n",
    "            for card in doc.select(\".card\"):\r\n",
    "                if limit_entries > 0 and entry_count >= limit_entries:\r\n",
    "                    url = None\r\n",
    "                    break\r\n",
    "                emoji = card.select_one(\".emoji\").text\r\n",
    "                content = card.select_one(\".card-text\").text\r\n",
    "                title = card.select(\".card-title span\")[1].text\r\n",
    "                image = urljoin(url, card.select_one(\"img\").attrs[\"src\"])\r\n",
    "                yield CrawledArticle(title, emoji, content, image)\r\n",
    "                entry_count += 1\r\n",
    "\r\n",
    "            page_count += 1\r\n",
    "            url = next_page.url\r\n",
    "\r\n",
    "    \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import csv\r\n",
    "\r\n",
    "fetcher = ArticleFetcher(\"http://python.beispiel.programmierenlernen.io/index.php\")\r\n",
    "\r\n",
    "with open(\"data\\crawler_output.csv\", 'w', newline='', encoding='utf-8') as csv_file:\r\n",
    "\r\n",
    "    article_writer = csv.writer(csv_file, delimiter = ';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\r\n",
    "    article_writer.writerow(CrawledArticle.csv_header())\r\n",
    "\r\n",
    "    for article in fetcher.fetch():\r\n",
    "        article_writer.writerow(article.to_csv())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "read data from http://python.beispiel.programmierenlernen.io/index.php\n",
      "read data from http://python.beispiel.programmierenlernen.io/index.php?page=2\n",
      "read data from http://python.beispiel.programmierenlernen.io/index.php?page=3\n",
      "read data from http://python.beispiel.programmierenlernen.io/index.php?page=4\n",
      "read data from http://python.beispiel.programmierenlernen.io/index.php?page=5\n",
      "read data from http://python.beispiel.programmierenlernen.io/index.php?page=6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}